<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <link rel="shortcut icon"  type="image/x-icon" href="/images/favicon.ico">
  <link rel="icon" type="image/x-icon" href="/images/favicon.ico">

  <title>Terracotta</title>

  <meta name="description" content="Java's most widely used cache.">

  <link rel="canonical" href="http://www.terracotta.org/documentation/4.1/terracotta-server-array/high-availability.html">
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Terracotta Feed">


  <!-- Fonts -->
  <link href='https://fonts.googleapis.com/css?family=Lato:300,400,300italic,400italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>

  <!-- Global CSS -->

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.5/spacelab/bootstrap.min.css">


<!--
  <link rel="stylesheet" href="/plugins/highlight/styles/idea.css">
  <script src="/plugins/highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
-->

  <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
  <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/prettify.css"></script>

  <link rel="stylesheet" href="/css/main.css">

<!--
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/prettify.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/prettify.min.js"></script>
  <script>document.addEventListener('DOMContentLoaded', prettyPrint)</script>
-->

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

</head>


  <body>

    

    <!-- Fixed navbar -->
    <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="site-title" href="/"><img src="/images/Terracotta_Logo_sm.png" style="margin-top:12px;margin-bottom:6px;"/></a>
          <span style="vertical-align:bottom;color:gray;">By</span>
          <a class="SAG-Logo" href="http://www.softwareag.com/corporate/products/terracotta"><img src="/images/SAG_Logo.png" style="margin-top:12px;margin-bottom:6px;height:1.5em;width:9em;"/></a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li id="tc_mnu_about"><a href="/about/"><i class="fa fa-info-circle"></i> About</a></li>
            <li id="tc_mnu_docs"><a href="/documentation/"><i class="fa fa-book"></i> Docs</a></li>
            <li id="tc_mnu_download"><a href="/downloads/"><i class="fa fa-download"></i> Download</a></li>
            <li id="tc_mnu_community" class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"><i class="fa fa-users"></i> Community <span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li class="dropdown-header">We Love Contributors</li>
                <li><a href="/community/contribute.html"><i class="fa fa-code"></i> Contributing</a></li>
                <li><a href="/resources/"><i class="fa fa-external-link-square"></i> External Resources</a></li>
                <li><a href="/blog" target="_blank"><i class="fa fa-rss-square"></i> Terracotta Blog</a></li>
                <li role="separator" class="divider"></li>
                <li class="dropdown-header">Forums</li>
                <li><a href="https://groups.google.com/forum/#!forum/terracotta-oss" target="_blank"><i class="fa fa-commenting"></i> Users' Forum</a></li>
                <li role="separator" class="divider"></li>
                <li class="dropdown-header">Source Code</li>
                <li><a href="https://github.com/Terracotta-OSS" target="_blank"><i class="fa fa-github"></i> GitHub  Repositories</a></li>
                <li><a href="http://svn.terracotta.org/svn/tc/" target="_blank"><i class="fa fa-code-fork"></i> SVN  (Terracotta 4.x)</a></li>
                <li role="separator" class="divider"></li>
                <li class="dropdown-header">Bug Tracking</li>
                <li><a href="https://github.com/Terracotta-OSS" target="_blank"><i class="fa fa-bug"></i> GitHub  (use respective project)</a></li>
              </ul>
            </li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li id="tc_mnu_events"><a href="/events"><i class="fa fa-calendar"></i> News & Events</a></li>
            <li><a href="/blog"><i class="fa fa-rss-square"></i> Terracotta Blog</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

<br/>
<br/>
<br/>


    <div class="container-fluid">
      <div id="contentTitle">
        <h1></h1>
      </div>
      <div>
        <BR/>
        <br/>

<div class="container-fluid">

  <div class="row row-offcanvas row-offcanvas-left">

    <!-- sidebar -->
    <div class="col-xs-6 col-sm-3 sidebar-offcanvas" id="sidebar" role="navigation">
        <ul class="nav">
          <li id="tc_mnu_docs_current" class="submenu"><a href="#current_version">Current Documentation</a></li>
          <li id="tc_mnu_doce_release_notes" class="submenu"><a href="http://www.terracotta.org/confluence/display/release/Home" target="_blank">Release Notes</a></li>
          <li id="tc_mnu_docs_previous" class="submenu"><a href="#historical_versions">Historical Versions</a></li>
        </ul>
    </div>

    <!-- main area -->
    <div class="col-xs-12 col-sm-9">
      <header class="post-header">
        
        <h1 class="post-title"></h1>
        <hr/>
        
      </header>
      <article class="post-content">
        <h1 id="configuring-terracotta-clusters-for-high-availability-{#19590}">Configuring Terracotta Clusters For High Availability {#19590}</h1><div id="toc-container">
   <table class="toc" id="toc">
      <tbody>
         <tr>
            <td>
               <ul>
                  <li class="toc_level-1 toc_section-1">
                     <a href="#configuring-terracotta-clusters-for-high-availability-{#19590}">
                        <span class="toctext">Configuring Terracotta Clusters For High Availability {#19590}</span>
                     </a>
                     <ul>
                        <li class="toc_level-2 toc_section-2">
                           <a href="#introduction">
                              <span class="toctext">Introduction</span>
                           </a>
                        </li>
                        <li class="toc_level-2 toc_section-3">
                           <a href="#basic-high-availability-configuration">
                              <span class="toctext">Basic High-Availability Configuration</span>
                           </a>
                        </li>
                        <li class="toc_level-2 toc_section-4">
                           <a href="#high-availability-features">
                              <span class="toctext">High-Availability Features</span>
                           </a>
                        </li>
                        <li class="toc_level-2 toc_section-5">
                           <a href="#effective-client-server-reconnection-settings:-an-example">
                              <span class="toctext">Effective Client-Server Reconnection Settings: An Example</span>
                           </a>
                        </li>
                        <li class="toc_level-2 toc_section-6">
                           <a href="#testing-high-availability-deployments">
                              <span class="toctext">Testing High-Availability Deployments</span>
                           </a>
                        </li>
                     </ul>
                  </li>
               </ul>
            </td>
         </tr>
      </tbody>
   </table>
</div>

<table>
  <tbody>
    <tr>
      <td>{toc</td>
      <td>2:3}</td>
    </tr>
  </tbody>
</table>

<h2 id="introduction">Introduction</h2>
<p>High Availability (HA) is an implementation designed to maintain uptime and access to services even during component overloads and failures. Terracotta clusters offer simple and scalable HA implementations based on the Terracotta Server Array (see <a href="server-arrays#67714">Terracotta Server Array Architecture</a> for more information).</p>

<p>The main features of a Terracotta HA architecture include:</p>

<ul>
  <li>Instant failover using a hot standby or multiple active servers –  provides continuous uptime and services</li>
  <li>Configurable automatic internode monitoring –  Terracotta <a href="#85916">HealthChecker</a></li>
  <li>Automatic permanent storage of all current shared (in-memory) data –  available to all server instances (no loss of application state)</li>
  <li>
    <p>Automatic reconnection of temporarily disconnected server instances and clients –  restores hot standbys without operator intervention, allows “lost” clients to reconnect</p>

    <p>Client reconnection refers to reconnecting clients that have not yet been disconnected from the cluster by the Terracotta Server Array. To learn about reconnecting BigMemory Max clients that have been disconnected from their cluster, see <a href="#71266">Using Rejoin to Automatically Reconnect Terracotta Clients</a>.</p>
  </li>
</ul>

<table>
<caption>TIP:  Nomenclature</caption>
<tr>
<td>
This document may refer to a Terracotta server instance as L2, and a Terracotta client (the node running your application) as L1. These are the shorthand references used in Terracotta configuration files.
</td>
</tr>
</table>

<p>It is important to thoroughly test any High Availability setup before going to production. Suggestions for testing High Availability configurations are provided in <a href="#testing-high-availability-deployments">this section</a>.</p>

<h2 id="basic-high-availability-configuration">Basic High-Availability Configuration</h2>

<p>A basic high-availability configuration has the following components:</p>

<ul>
  <li>
    <p>Two or More Terracotta Server Instances</p>

    <p>You may set up High Availability using either <code class="highlighter-rouge">&lt;server&gt;</code> or <code class="highlighter-rouge">&lt;mirror-group&gt;</code> configurations. Note that <code class="highlighter-rouge">&lt;server&gt;</code> instances do work together as a mirror group, but to create more than one stripe, or to configure the election-time, use <code class="highlighter-rouge">&lt;mirror-group&gt;</code> instances. See <a href="server-arrays#67714">Terracotta Server Arrays Architecture</a> on how to set up a cluster with multiple Terracotta server instances.</p>
  </li>
  <li>
    <p>Server-Server Reconnection</p>

    <p>A reconnection mechanism can be enabled to restore lost connections between active and mirror Terracotta server instances. See <a href="#41216">Automatic Server Instance Reconnect</a> for more information.</p>
  </li>
  <li>
    <p>Server-Client Reconnection</p>

    <p>A reconnection mechanism can be enabled to restore lost connections between Terracotta clients and server instances. See <a href="#77332">Automatic Client Reconnect</a> for more information.</p>
  </li>
</ul>

<h2 id="high-availability-features">High-Availability Features</h2>

<p>The following high-availability features can be used to extend the reliability of a Terracotta cluster. These features are controlled using properties set with the &lt;tc-properties&gt; section at the <em>beginning</em> of the Terracotta configuration file:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>&lt;tc:tc-config xmlns:tc="http://www.terracotta.org/config"  
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  
xsi:schemaLocation="http://www.terracotta.org/schema/terracotta-9.xsd"&gt;

 &lt;tc-properties&gt;
   &lt;property name="some.property.name" value="true"/&gt;
   &lt;property name="some.other.property.name" value="true"/&gt;
   &lt;property name="still.another.property.name" value="1024"/&gt;
 &lt;/tc-properties&gt;

&lt;!-- The rest of the Terracotta configuration goes here. --&gt;

&lt;/tc:tc-config&gt;
</code></pre>
</div>

<p>See the section <em>Overriding tc.properties</em> in <a href="/documentation/4.1/terracotta-server-array/config-reference#overriding-tcproperties">Configuration Guide and Reference</a> for more information.</p>

<h3 id="healthchecker-85916">HealthChecker {#85916}</h3>

<p>HealthChecker is a connection monitor similar to TCP keep-alive. HealthChecker functions between Terracotta server instances (in High Availability environments), and between Terracotta sever instances and clients. Using HealthChecker, Terracotta nodes can determine if peer nodes are reachable, up, or in a GC operation. If a peer node is unreachable or down, a Terracotta node using HealthChecker can take corrective action. HealthChecker is on by default.</p>

<p>You configure HealthChecker using certain Terracotta properties, which are grouped into three different categories:</p>

<ul>
  <li>Terracotta server instance -&gt; Terracotta client</li>
  <li>Terracotta Server -&gt; Terracotta Server (HA setup only)</li>
  <li>Terracotta Client -&gt; Terracotta Server</li>
</ul>

<p>Property category is indicated by the prefix:</p>

<ul>
  <li>l2.healthcheck.l1 indicates L2 -&gt; L1</li>
  <li>l2.healthcheck.l2 indicates L2 -&gt; L2</li>
  <li>l1.healthcheck.l2 indicates L1 -&gt; L2</li>
</ul>

<p>For example, the <code class="highlighter-rouge">l2.healthcheck.l2.ping.enabled</code> property applies to L2 -&gt; L2.</p>

<p>The following HealthChecker properties can be set in the &lt;tc-properties&gt; section of the Terracotta configuration file:</p>

<table>
<tr>
<th>Property</th>
<th>Definition</th>
</tr>
<tr>
<td>
<code>l2.healthcheck.l1.ping.enabled</code>

<code>l2.healthcheck.l2.ping.enabled</code>

<code>l1.healthcheck.l2.ping.enabled</code>
</td>
<td>
Enables (True) or disables (False) ping probes (tests). Ping probes are high-level attempts to gauge the ability of a remote node to respond to requests and is useful for determining if temporary inactivity or problems are responsible for the node's silence. Ping probes may fail due to long GC cycles on the remote node.
</td>
</tr>
<tr>
<td>
<code>l2.healthcheck.l1.ping.idletime</code>

<code>l2.healthcheck.l2.ping.idletime</code>

<code>l1.healthcheck.l2.ping.idletime</code>
</td>
<td>
The maximum time (in milliseconds) that a node can be silent (have no network traffic) before HealthChecker begins a ping probe to determine if the node is alive.
</td>
</tr>
<tr>
<td>
<code>l2.healthcheck.l1.ping.interval</code>

<code>l2.healthcheck.l2.ping.interval</code>

<code>l1.healthcheck.l2.ping.interval</code>
</td>
<td>
If no response is received to a ping probe, the time (in milliseconds) that HealthChecker waits between retries.
</td>
</tr>
<tr>
<td>
<code>l2.healthcheck.l1.ping.probes</code>

<code>l2.healthcheck.l2.ping.probes</code>

<code>l1.healthcheck.l2.ping.probes</code>
</td>
<td>
If no response is received to a ping probe, the maximum number (integer) of retries HealthChecker can attempt.
</td>
</tr>
<tr>
<td>
<code>l2.healthcheck.l1.socketConnect</code>

<code>l2.healthcheck.l2.socketConnect</code>

<code>l1.healthcheck.l2.socketConnect</code>
</td>
<td>
Enables (True) or disables (False) socket-connection tests. This is a low-level connection that determines if the remote node is reachable and can access the network. Socket connections are not affected by GC cycles.
</td>
</tr>
<tr>
<td>
<code>l2.healthcheck.l1.socketConnectTimeout</code>

<code>l2.healthcheck.l2.socketConnectTimeout</code>

<code>l1.healthcheck.l2.socketConnectTimeout</code>
</td>
<td>
A multiplier (integer) to determine the maximum amount of time that a remote node has to respond before HealthChecker concludes that the node is dead (regardless of previous successful socket connections). The time is determined by multiplying the value in ping.interval by this value.
</td>
</tr>
<tr>
<td>
<code>l2.healthcheck.l1.socketConnectCount</code>

<code>l2.healthcheck.l2.socketConnectCount</code>

<code>l1.healthcheck.l2.socketConnectCount</code>
</td>
<td>
The maximum number (integer) of successful socket connections that can be made without a successful ping probe. If this limit is exceeded, HealthChecker concludes that the target node is dead.
</td>
</tr>
<tr>
<td>
<code>l1.healthcheck.l2.bindAddress</code>
</td>
<td>
Binds the client to the configured IP address. This is useful where a host has more than one IP address available for a client to use. The default value of "0.0.0.0" allows the system to assign an IP address.
</td>
</tr>
<tr>
<td>
<code>l1.healthcheck.l2.bindPort</code>
</td>
<td>
Set the client's callback port. Terracotta configuration does not assign clients a port for listening to cluster communications such as that required by HealthChecker. The default value of "0" allows the system to assign a port. To specify a port number or a range of ports, you can provide a port number as the value, or a comma separated list of ports where each element of the list can either be a single number or range. A value of "-1" disables a client's callback port.
</td>
</tr>
</table>

<p>The following diagram illustrates how HealthChecker functions.</p>

<p>
   <img src="/images/documentation/HealthCheckerFlow.png" alt="Terracotta HealthChecker flow diagram." />
</p>

<h4 id="calculating-healthchecker-maximum">Calculating HealthChecker Maximum</h4>

<p>The following formula can help you compute the maximum time it will take HealthChecker to discover failed or disconnected remote nodes:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Max Time = (ping.idletime) + socketConnectCount * [(ping.interval * ping.probes)
           + (socketConnectTimeout * ping.interval)]
</code></pre>
</div>

<p>Note the following about the formula:</p>

<ul>
  <li>
    <p>The response time to a socket-connection attempt is less than or equal to <code class="highlighter-rouge">(socketConnectTimeout * ping.interval)</code>. For calculating the worst-case scenario (absolute maximum time), the equality is used. In most real-world situations the socket-connect response time is likely to be close to 0 and the formula can be simplified to the following:</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>  Max Time = (ping.idletime) + [socketConnectCount * (ping.interval * ping.probes)]
</code></pre>
    </div>
  </li>
  <li><code class="highlighter-rouge">ping.idletime</code>, the trigger for the full HealthChecker process, is counted once since it is in effect only once each time the process is triggered.</li>
  <li><code class="highlighter-rouge">socketConnectCount</code> is a multiplier that is in incremented as long as a positive response is received for each socket connection attempt.</li>
  <li>The formula yields an ideal value, since slight variations in actual times can occur.</li>
  <li>To prevent clients from disconnecting too quickly in a situation where an active server is temporarily disconnected from both the backup server and those clients, ensure that the Max Time for L1-&gt;L2 is approximately 8-12 seconds longer than for L2-&gt;L2. If the values are too close together, then in certain situations the active server may kill the backup and refuse to allow clients to reconnect.</li>
</ul>

<h4 id="configuration-examples">Configuration Examples</h4>

<p>The configuration examples in this section show settings for L1 -&gt; L2 HealthChecker. However, they apply in the similarly to L2 -&gt; L2 and L2 -&gt; L1, which means that the server is using HealthChecker on the client.</p>

<h5 id="aggressive">Aggressive</h5>

<p>The following settings create an aggressive HealthChecker with low tolerance for short network outages or long GC cycles:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>&lt;property name="l1.healthcheck.l2.ping.enabled" value="true" /&gt;
&lt;property name="l1.healthcheck.l2.ping.idletime" value="2000" /&gt;
&lt;property name="l1.healthcheck.l2.ping.interval" value="1000" /&gt;
&lt;property name="l1.healthcheck.l2.ping.probes" value="3" /&gt;
&lt;property name="l1.healthcheck.l2.socketConnect" value="true" /&gt;
&lt;property name="l1.healthcheck.l2.socketConnectTimeout" value="2" /&gt;
&lt;property name="l1.healthcheck.l2.socketConnectCount" value="5" /&gt;
</code></pre>
</div>

<p>According to the HealthChecker “Max Time” formula, the maximum time before a remote node is considered to be lost is computed in the following way:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>2000 + 5 [( 3 * 1000 ) + ( 2 * 1000)] = 27000
</code></pre>
</div>

<p>In this case, after the initial idletime of 2 seconds, the remote failed to respond to ping probes but responded to every socket connection attempt, indicating that the node is reachable but not functional (within the allowed time frame) or in a long GC cycle. This aggressive HealthChecker configuration declares a node dead in no more than 27 seconds.</p>

<p>If at some point the node had been completely unreachable (a socket connection attempt failed), HealthChecker would have declared it dead sooner. Where, for example, the problem is a disconnected network cable, the “Max Time” is likely to be even shorter:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>2000 + 1[3 * 1000) + ( 2 * 1000 ) = 7000
</code></pre>
</div>

<p>In this case, HealthChecker declares a node dead in no more than 7 seconds.</p>

<h5 id="tolerant">Tolerant</h5>

<p>The following settings create a HealthChecker with a higher tolerance for interruptions in network communications and long GC cycles:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>&lt;property name="l1.healthcheck.l2.ping.enabled" value="true" /&gt;
&lt;property name="l1.healthcheck.l2.ping.idletime" value="5000" /&gt;
&lt;property name="l1.healthcheck.l2.ping.interval" value="1000" /&gt;
&lt;property name="l1.healthcheck.l2.ping.probes" value="3" /&gt;
&lt;property name="l1.healthcheck.l2.socketConnect" value="true" /&gt;
&lt;property name="l1.healthcheck.l2.socketConnectTimeout" value="5" /&gt;
&lt;property name="l1.healthcheck.l2.socketConnectCount" value="10" /&gt;
</code></pre>
</div>

<p>According to the HealthChecker “Max Time” formula, the maximum time before a remote node is considered to be lost is computed in the following way:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>5000 + 10 [( 3 x 1000 ) + ( 5 x 1000)] = 85000
</code></pre>
</div>

<p>In this case, after the initial idletime of 5 seconds, the remote failed to respond to ping probes but responded to every socket connection attempt, indicating that the node is reachable but not functional (within the allowed time frame) or excessively long GC cycle. This tolerant HealthChecker configuration declares a node dead in no more than 85 seconds.</p>

<p>If at some point the node had been completely unreachable (a socket connection attempt failed), HealthChecker would have declared it dead sooner. Where, for example, the problem is a disconnected network cable, the “Max Time” is likely to be even shorter:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>5000 + 1[3 * 1000) + ( 5 * 1000 )] = 13000
</code></pre>
</div>

<p>In this case, HealthChecker declares a node dead in no more than 13 seconds.</p>

<h4 id="tuning-healthchecker-to-allow-for-gc-or-network-interruptions">Tuning HealthChecker to Allow for GC or Network Interruptions</h4>

<p>GC cycles do not affect a node’s ability to respond to socket-connection requests, while network interruptions do. This difference can be used to tune HealthChecker to work more efficiently in a cluster where one or the other of these issues is more likely to occur:</p>

<ul>
  <li>
    <p>To favor detection of network interruptions, adjust the socketConnectCount down (since socket connections will fail). This will allow HealthChecker to disconnect a client sooner due to network issues.</p>
  </li>
  <li>
    <p>To favor detection of GC pauses, adjust the socketConnectCount up (since socket connections will succeed). This will allow clients to remain in the cluster longer when no network disconnection has occurred.</p>
  </li>
</ul>

<p>The ping interval increases the time before socket-connection attempts kick in to verify health of a remote node. The ping interval can be adjusted up or down to add more or less tolerance in either of the situations listed above.</p>

<h3 id="automatic-server-instance-reconnect-41216">Automatic Server Instance Reconnect {#41216}</h3>

<p>An automatic reconnect mechanism can prevent short network disruptions from forcing a restart for any Terracotta server instances in a server array with hot standbys. If not disabled, this mechanism is by default in effect in clusters set to networked-based HA mode.</p>

<table>
<caption>NOTE:  Increased Time-to-Failover</caption>
<tr>
<td>
This feature increases time-to-failover by the timeout value set for the automatic reconnect mechanism.
</td>
</tr>
</table>

<p>This event-based reconnection mechanism works independently and exclusively of HealthChecker. If HealthChecker has already been triggered, this mechanism cannot be triggered for the same node. If this mechanism is triggered first by an internal Terracotta event, HealthChecker is prevented from being triggered for the same node. The events that can trigger this mechanism are not exposed by API but are logged.</p>

<p>Configure the following properties for the reconnect mechanism:</p>

<ul>
  <li><code class="highlighter-rouge">l2.nha.tcgroupcomm.reconnect.enabled</code> –  (DEFAULT: false) When set to “true” enables a server instance to attempt reconnection with its peer server instance after a disconnection is detected. Most use cases should benefit from enabling this setting.</li>
  <li><code class="highlighter-rouge">l2.nha.tcgroupcomm.reconnect.timeout</code> –  Enabled if <code class="highlighter-rouge">l2.nha.tcgroupcomm.reconnect.enabled</code> is set to true. Specifies the timeout (in milliseconds) for reconnection. Default: 2000. This parameter can be tuned to handle longer network disruptions.</li>
</ul>

<h3 id="automatic-client-reconnect-77332">Automatic Client Reconnect {#77332}</h3>

<p>Clients disconnected from a Terracotta cluster normally require a restart to reconnect to the cluster. An automatic reconnect mechanism can prevent short network disruptions from forcing a restart for Terracotta clients disconnected from a Terracotta cluster.</p>

<table>
<caption>NOTE:  Performance Impact of Using Automatic Client Reconnect</caption>
<tr>
<td>
With this feature, clients waiting to reconnect continue to hold locks. Some application threads may block while waiting to for the client to reconnect.
</td>
</tr>
</table>

<p>This event-based reconnection mechanism works independently and exclusively of HealthChecker. If HealthChecker has already been triggered, this mechanism cannot be triggered for the same node. If this mechanism is triggered first by an internal Terracotta event, HealthChecker is prevented from being triggered for the same node. The events that can trigger this mechanism are not exposed by API but are logged.</p>

<p>Configure the following properties for the reconnect mechanism:</p>

<ul>
  <li><code class="highlighter-rouge">l2.l1reconnect.enabled</code> – (DEFAULT: false) When set to “true” enables a client to reconnect to a cluster after a disconnection is detected. This property controls a server instance’s reaction to such an attempt. It is set on the server instance and is passed to clients by the server instance. A client cannot override the server instance’s setting. If a mismatch exists between the client setting and a server instance’s setting, and the client attempts to reconnect to the cluster, the client emits a mismatch error and exits. Most use cases should benefit from enabling this setting.</li>
  <li><code class="highlighter-rouge">l2.l1reconnect.timeout.millis</code> –  Enabled if <code class="highlighter-rouge">l2.l1reconnect.enabled</code> is set to true. Specifies the timeout (in milliseconds) for reconnection. This property controls a server instance’s timeout during such an attempt. It is set on the server instance and is passed to clients by the server instance. A client cannot override the server instance’s setting. Default: 2000. This parameter can be tuned to handle longer network disruptions.</li>
</ul>

<h3 id="special-client-connection-properties">Special Client Connection Properties</h3>

<p>Client connections can also be tuned for the following special cases:</p>

<ul>
  <li>Client failover after server failure</li>
  <li>First-time client connection</li>
</ul>

<p>The connection properties associated with these cases are already optimized for most typical environments. If you attempt to tune these properties, be sure to thoroughly test the new settings.</p>

<h4 id="client-failover-after-server-failure">Client Failover After Server Failure</h4>

<p>When an active Terracotta server instance fails, and a mirror Terracotta server is available, the mirror server becomes active. Terracotta clients connected to the previous active server automatically switch to the new active server. However, these clients have a limited window of time to complete the failover.</p>

<table>
<caption>TIP: Clusters with a Single Server</caption>
<tr>
<td>
This reconnection window also applies in a cluster with a single Terracotta server that is restarted. However, a single-server cluster must have &lt;restartable&gt; enabled for the reconnection window to take effect.
</td>
</tr>
</table>

<p>This window is configured in the Terracotta configuration file using the &lt;client-reconnect-window&gt; element:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>&lt;servers&gt;
  ...
  &lt;client-reconnect-window&gt;120&lt;/client-reconnect-window&gt;
  &lt;!-- The reconnect window is configured in seconds, with a default value of 120.
       The default value is "built in," so the element does not have to be explicitly
       added unless a different value is required. --&gt;
  ...
&lt;/servers&gt;
</code></pre>
</div>

<p>Clients that fail to connect to the new active server must be restarted if they are to successfully reconnect to the cluster.</p>

<h4 id="first-time-client-connection-98653">First-Time Client Connection {#98653}</h4>

<p>When a Terracotta client is first started (or restarted), it attempts to connect to a Terracotta server instance based on the following properties:</p>

<div class="highlighter-rouge"><pre class="highlight"><code># Must the client and server be running the same version of Terracotta?
l1.connect.versionMatchCheck.enabled = true
# Time (in milliseconds) before a socket connection attempt is timed out.
l1.socket.connect.timeout=10000
# Time (in milliseconds; minimum 10) between attempts to connect to a server.
l1.socket.reconnect.waitInterval=1000
</code></pre>
</div>

<p>The timing properties above control connection attempts only <em>after</em> the client has obtained and resolved its configuration from the server. To control connection attempts <em>before</em> configuration is resolved, set the following property on the client:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>-Dcom.tc.tc.config.total.timeout=5000
</code></pre>
</div>

<p>This property limits the time (in milliseconds) that a client spends attempting to make an initial connection.</p>

<h3 id="using-rejoin-to-automatically-reconnect-terracotta-clients-71266">Using Rejoin to Automatically Reconnect Terracotta Clients {#71266}</h3>

<p>A Terracotta client may disconnect and be timed out (ejected) from the cluster. Typically, this occurs because of network communication interruptions lasting longer than the configured HA settings for the cluster. Other causes include long GC pauses and slowdowns introduced by other processes running on the client hardware.</p>

<p>You can configure clients to automatically rejoin a cluster after they are ejected. If the ejected client continues to run under nonstop cache settings, and then senses that it has reconnected to the cluster (receives a clusterOnline event), it can begin the rejoin process.</p>

<p>Note the following about using the rejoin feature:</p>

<ul>
  <li>Disconnected clients can only rejoin clusters to which they were previously connected.</li>
  <li>Clients rejoin as new members and will wipe all cached data to ensure that no pauses or inconsistencies are introduced into the cluster.</li>
  <li>Clients cannot rejoin a new cluster; if the TSA has been restarted and its data has not been persisted, clients can never rejoin and must be restarted.</li>
  <li>If the TSA has been restarted and its data has been persisted, clients are allowed to rejoin.</li>
  <li>Any nonstop-related operations that begin (and do not complete) before the rejoin operation completes may be unsuccessful and may generate a NonStopCacheException.</li>
  <li>If a Terracotta client with rejoin enabled is running in a JVM with clients that do not have rejoin, then only that client will rejoin after a disconnection. The remaining clients cannot rejoin and may cause the application to behave unpredictably.</li>
  <li>Once a client rejoins, the clusterRejoined event is fired on that client only.</li>
</ul>

<h4 id="configuring-rejoin">Configuring Rejoin</h4>

<p>The rejoin feature is disabled by default. To enable the rejoin feature in an Terracotta client, follow these steps:</p>

<ol>
  <li>Ensure that all of the caches in the Ehcache configuration file where rejoin is enabled have nonstop enabled.</li>
  <li>Ensure that your application does not create caches on the client without nonstop enabled.</li>
  <li>
    <p>Enable the rejoin attribute in the client’s <code class="highlighter-rouge">&lt;terracottaConfig&gt;</code> element:</p>

    <div class="highlighter-rouge"><pre class="highlight"><code> &lt;terracottaConfig url="myHost:9510" rejoin="true" /&gt;
</code></pre>
    </div>
  </li>
</ol>

<p>For more options on configuring <code class="highlighter-rouge">&lt;terracottaConfig&gt;</code>, see the <a href="/documentation/4.1/bigmemorymax/configuration/distributed-configuration#35085">configuration reference</a>.</p>

<h4 id="exception-during-rejoin">Exception During Rejoin</h4>

<p>Under certain circumstances, if a lock is being used by your application, an InvalidLockAfterRejoinException could be thrown during or after client rejoin. This exception occurs when an unlock operation takes place on a lock obtained <em>before</em> the rejoin attempt completed.</p>

<p>To ensure that locks are released properly, application code should encapsulate lock-unlock operations with try-finally blocks:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>myLock.acquireLock();
try {
  // Do some work.
} finally {
  myLock.unlock();
}
</code></pre>
</div>

<h2 id="effective-client-server-reconnection-settings:-an-example">Effective Client-Server Reconnection Settings: An Example</h2>

<p>To prevent unwanted disconnections, it is important to understand the potentially complex interaction between HA settings and the environment in which your cluster runs. Settings that are not appropriate for a particular environment can lead to unwanted disconnections under certain circumstances.</p>

<p>In general, it is advisable to maintain an L1-L2 HealthChecker timeout that falls between the L2-L2 HealthChecker timeout as modified in the following ineqaulity:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>L2-L2 HealthCheck + Election Time
        &lt;  L1-L2 HealthCheck
                  &lt;  L2-L2 HealthCheck + Election Time + Client Reconnect Window
</code></pre>
</div>

<p>This allows a cluster’s L1s to avoid disconnecting before a client reconnection window is opened (a backup L2 takes over), or to not disconnect if that window is never opened (the original active L2 is still functional). The Election Time and Client Reconnect Window settings, which are found in the Terracotta configuration file, are respectively 5 seconds and 120 seconds by default.</p>

<p>For example, in a cluster where the L2-L2 HealthChecker triggers at 55 seconds, a backup L2 can take over the cluster after 180 seconds (55 + 5 + 120). If the L1-L2 HealthChecker triggers after a time that is greater than 180 seconds, clients may not attempt to reconnect until the reconnect window is closed and it’s too late.</p>

<p>If the L1-L2 HealthChecker triggers after a time that is less than 60 seconds (L2-L2 HealthChecker + Election Time), then the clients may disconnect from the active L2 before its failure is determined. Should the active L2 win the election, the disconnected L1s would then be lost.</p>

<p>A check is performed at server startup to ensure that L1-L2 HealthChecker settings are within the effective range. If not, a warning with a prescription is printed.</p>

<h2 id="testing-high-availability-deployments">Testing High-Availability Deployments</h2>
<p>This section presents recommendations for designing and testing a robust cluster architecture. While these recommendations have been tested and shown to be effective under certain circumstances, in your custom environment additional testing is still necessary to ensure an optimal setup, and to meet specific demands related to factors such as load.</p>

<h3 id="high-availability-network-architecture-and-testing">High-Availability Network Architecture And Testing</h3>

<p>To take advantage of the Terracotta active-mirror server configuration, certain network configurations are necessary to prevent split-brain scenarios and ensure that Terracotta clients (L1s) and server instances (L2s) behave in a deterministic manner after a failure occurs. This is regardless of the nature of the failure, whether network, machine, or other type.</p>

<p>This section outlines two possible network configurations that are known to work with Terracotta failover.  While it is possible for other network configurations to work reliably, the configurations listed in this document have been well tested and are fully supported.</p>

<h3 id="deployment-configuration-simple-no-network-redundancy"><em>Deployment Configuration</em>: Simple (no network redundancy)</h3>
<p>
   <img src="/images/documentation/TC.single.NIC.png" alt="IMAGE: Simple Network Setup" />
</p>

<h4 id="description">Description</h4>
<p>This is the simplest network configuration.  There is no network redundancy so when any failure occurs, there is a good chance that all or part of the cluster will stop functioning.  All failover activity is up to the Terracotta software.</p>

<p>In this diagram, the IP addresses are merely examples to demonstrate that the L1s (L1a &amp; L1b) and L2s (TCserverA &amp; TCserverB) can live on different subnets. The actual addressing scheme is specific to your environment. The single switch is a single point of failure.</p>

<h4 id="additional-configuration">Additional configuration</h4>

<p>There is no additional network or operating-system configuration necessary in this configuration. Each machine needs a proper network configuration (IP address, subnet mask, gateway, DNS, NTP, hostname) and must be plugged into the network.</p>

<h4 id="test-plan---network-failures-non-redundant-network">Test Plan - Network Failures Non-Redundant Network</h4>

<p>To determine that your configuration is correct, use the following tests to confirm all failure scenarios behave as expected.</p>

<table>
<tr><th>TestID</th><th> Failure </th><th>  Expected Outcome  </th></tr>
<tr><td>FS1</td><td> Loss of L1a (link or system)</td><td> Cluster continues as normal using only L1b </td></tr>
<tr><td>FS2</td><td> Loss of L1b (link or system)</td><td> Cluster continues as normal using only L1a </td></tr>
<tr><td>FS3</td><td> Loss of L1a &amp; L1b </td><td> Non-functioning cluster </td></tr>
<tr><td>FS4</td><td> Loss of Switch    </td><td> Non-functioning cluster </td></tr>
<tr><td>FS5</td><td> Loss of Active L2 (link or system) </td><td> mirror L2 becomes new Active L2, L1s fail over to new Active L2 </td></tr>
<tr><td>FS6</td><td> Loss of mirror L2 </td><td> Cluster continues as normal without TC redundancy </td></tr>
<tr><td>FS7</td><td> Loss of TCservers A &amp; B </td><td> Non-functioning cluster </td></tr>
</table>

<h4 id="test-plan---network-tests-non-redundant-network">Test Plan - Network Tests Non-redundant Network</h4>

<p>After the network has been configured, you can test your configuration with simple ping tests.</p>

<table>
<tr><th>TestID</th><th> Host </th><th> Action </th><th> Expected Outcome </th></tr>
<tr><td>NT1</td><td> all  </td><td> ping every other host </td><td> successful ping </td></tr>
<tr><td>NT2</td><td> all  </td><td> pull network cable during continuous ping </td><td> ping failure until link restored </td></tr>
<tr><td>NT3</td><td> switch </td><td> reload </td><td> all pings cease until reload complete and links restored </td></tr>
</table>

<h3 id="deployment-configuration-fully-redundant"><em>Deployment Configuration</em>: Fully Redundant</h3>

<p>
   <img src="/images/documentation/TC.redundant.NIC.png" alt="IMAGE:Fully Redundant Deployment Configuration" />
</p>

<h4 id="description-1">Description</h4>

<p>This is the fully redundant network configuration. It relies on the failover capabilities of Terracotta, the switches, and the operating system.  In this scenario it is even possible to sustain certain double failures and still maintain a fully functioning cluster.</p>

<p>In this diagram, the IP addressing scheme is merely to demonstrate that the L1s (L1a &amp; L1b) can be on a different subnet than the L2s (TCserverA &amp; TCserverB).  The actual addressing scheme will be specific to your environment.  If you choose to implement with a single subnet, then there will be no need for VRRP/HSRP but you will still need to configure a single VLAN (can be VLAN 1) for all TC cluster machines.</p>

<p>In this diagram, there are two switches that are connected with trunked links for redundancy and which implement Virtual Router Redundancy Protocol (VRRP) or HSRP to provide redundant network paths to the cluster servers in the event of a switch failure.  Additionally, all servers are configured with both a primary and secondary network link which is controlled by the operating system.  In the event of a NIC or link failure on any single link, the operating system should fail over to the backup link without disturbing (e.g. restarting) the Java processes (L1 or L2) on the systems.</p>

<p>The Terracotta fail over is identical to that in the simple case above, however both NIC cards on a single host would need to fail in this scenario before the TC software initiates any fail over of its own.</p>

<h4 id="additional-configuration-1">Additional configuration</h4>

<ul>
  <li>
    <p>Switch - Switches need to implement VRRP or HSRP to provide redundant gateways for each subnet.  Switches also need to have a trunked connection of two or more lines  in order to prevent any single link failure from splitting the virtual router in two.</p>
  </li>
  <li>
    <p>Operating System - Hosts need to be configured with bonded network interfaces connected to the two different switches.  For Linux, choose mode 1.  More information about Linux channel bonding can be found in the <a href="https://www.redhat.com/docs/manuals/enterprise/RHEL-4-Manual/en-US/Reference_Guide/s2-modules-bonding.html">RedHat Linux Reference Guide</a>. Pay special attention to the amount of time it takes for your VRRP or HSRP implementation to reconverge after a recovery.  You don’t want your NICs to change to a switch that is not ready to pass traffic.  This should be tunable in your bonding configuration.</p>
  </li>
</ul>

<h4 id="test-plan---network-failures-redundant-network">Test Plan - Network Failures Redundant Network</h4>

<p>The following tests continue the tests listed in Network Failures (Pt. 1).  Use these tests to confirm that your network is configured properly.</p>

<table>
<tr><th>TestID</th><th>Failure </th><th> Expected Outcome </th></tr>
<tr><td>FS8</td><td> Loss of any primary network link </td><td> Failover to standby link </td></tr>
<tr><td>FS9</td><td> Loss of all primary links </td><td> All nodes fail to their secondary link</td></tr>
<tr><td>FS10</td><td> Loss of any switch </td><td> Remaining switch assumes VRRP address and switches fail over NICs if necessary </td></tr>
<tr><td>FS11</td><td> Loss of any L1 (both links or system) </td><td> Cluster continues as normal using only other L1 </td></tr>
<tr><td>FS12</td><td> Loss of Active L2 </td><td> mirror L2 becomes the new Active L2, All L1s fail over to the new Active L2 </td></tr>
<tr><td>FS13</td><td> Loss of mirror L2 </td><td> Cluster continues as normal without TC redundancy </td></tr>
<tr><td>FS14</td><td> Loss of both switches </td><td> non-functioning cluster </td></tr>
<tr><td>FS15</td><td> Loss of single link in switch trunk </td><td> Cluster continues as normal without trunk redundancy </td></tr>
<tr><td>FS16</td><td> Loss of both trunk links </td><td> possible non-functioning cluster depending on VRRP or HSRP implementation </td></tr>
<tr><td>FS17</td><td> Loss of both L1s </td><td> non-functioning cluster </td></tr>
<tr><td>FS18</td><td> Loss of both L2s </td><td> non-functioning cluster </td></tr>
</table>

<h4 id="test-plan---network-testing-redundant-network">Test Plan - Network Testing Redundant Network</h4>

<p>After the network has been configured, you can test your configuration with simple ping tests and various failure scenarios.</p>

<p>The test plan for Network Testing consists of the following tests:</p>

<table>
<tr><th>TestID</th><th> Host </th><th> Action </th><th> Expected Outcome </th></tr>
<tr><td>NT4</td><td> any </td><td> ping every other host </td><td> successful ping </td></tr>
<tr><td>NT5</td><td> any </td><td> pull primary link during continuous ping to any other host </td><td> failover to secondary link, no noticable network interruption </td></tr>
<tr><td>NT6</td><td> any </td><td> pull standby link during continuous ping to any other host </td><td> no effect </td></tr>
<tr><td>NT7</td><td> Active L2 </td><td> pull both network links </td><td> mirror L2 becomes Active, L1s fail over to new Active L2 </td></tr>
<tr><td>NT8</td><td> Mirror L2 </td><td> pull both network links </td><td> no effect </td></tr>
<tr><td>NT9</td><td> switchA </td><td> reload </td><td> nodes detect link down and fail to standby link, brief network outage if VRRP transition occurs </td></tr>
<tr><td>NT10</td><td> switchB </td><td> reload </td><td>  brief network outage if VRRP transition occurs </td></tr>
<tr><td>NT11</td><td> switch </td><td> pull single trunk link </td><td> no effect </td></tr>
</table>

<h3 id="terracotta-cluster-tests">Terracotta Cluster Tests</h3>

<p>All tests in this section should be run after the Network Tests succeed.</p>

<h4 id="test-plan---active-l2-system-loss-tests---verify-mirror-takeover">Test Plan - Active L2 System Loss Tests - verify Mirror Takeover</h4>

<p>The test plan for mirror takeover consists of the following tests:</p>

<table>
<tr><th>TestID</th><th> Test </th><th> Setup </th><th> Steps </th><th> Expected Result </th></tr>
<tr><td>TAL1</td><td>Active L2 Loss - Kill </td><td>L2-A is active, L2-B is mirror. All systems are running and available to take traffic.</td><td>1. Run app<br />2. Kill -9 Terracotta PID on L2-A (Active)</td><td>L2-B(mirror) becomes active.  Takes the load. No drop in TPS on Failover.</td></tr>
<tr><td>TAL2</td><td>Active L2 Loss - clean shutdown</td><td> L2-A is active, L2-B is mirror. All systems are running and available to take traffic.</td><td>1. Run app   2.Run ~/bin/stop-tc-server.sh on L2-A (Active)</td><td>L2-B(mirror) becomes active.  Takes the load. No drop in TPS on Failover. </td></tr>
<tr><td>TAL3</td><td>Active L2 Loss - Power Down         </td><td> L2-A is Active, L2-B is mirror. All systems are running and available to take traffic</td><td>1. Run app     2. Power down L2-A (Active)</td><td>L2-B(mirror) becomes active.  Takes the load. No drop in TPS on Failover. </td></tr>
<tr><td>TAL4</td><td>Active L2 Loss - Reboot               </td><td>L2-A is Active, L2-B is mirror. All systems are running and available to take traffic</td><td>1. Run app    2. Reboot L2-A (Active)</td><td>L2-B(mirror) becomes active.  Takes the load. No drop in TPS on Failover. </td></tr>
<tr><td>TAL5</td><td>Active L2 Loss - Pull Plug               </td><td> L2-A is Active, L2-B is mirror. All systems are running and available to take traffic</td><td>1. Run app    2. Pull the power cable on L2-A (Active)</td><td>L2-B(mirror) becomes active.  Takes the load. No drop in TPS on Failover. </td></tr>
</table>

<h4 id="test-plan---mirror-l2-system-loss-tests">Test Plan - Mirror L2 System Loss Tests</h4>

<p>System loss tests confirms High Availability in the event of loss of a single system. This section outlines tests for testing failure of the Terracotta mirror server.</p>

<p>The test plan for testing Terracotta mirror Failures consist of the following tests:</p>

<table>
<tr><th>TestID</th><th> Test </th><th> Setup </th><th> Steps </th><th> Expected Result </th></tr>
<tr><td>TPL1</td><td>Mirror L2 loss   - kill                 </td><td> L2-A is active, L2-B is mirror. All systems are running and available to take traffic.</td><td>1. Run app  2. Kill -9 L2-B (mirror)</td><td>data directory needs to be cleaned up, then when L2-B is restarted,  it re-synchs state from Active Server.</td></tr>
<tr><td>TPL2</td><td>Mirror L2 loss   -clean                      </td><td> L2-A is active, L2-B is mirror. All systems are running and available to take traffic</td><td>1. Run app  2. Run ~/bin/stop-tc-server.sh on L2-B (mirror)</td><td>data directory needs to be cleaned up, then when L2-B is restarted,  it re-synchs state from Active Server.</td></tr>
<tr><td>TPL3</td><td>Mirror L2 loss   -power down                     </td><td> L2-A is active, L2-B is mirror. All systems are running and available to take traffic</td><td>1. Run app     2. Power down L2-B (mirror)</td><td>data directory needs to be cleaned up, then when L2-B is restarted,  it re-synchs state from Active Server.</td></tr>
<tr><td>TPL4</td><td>Mirror L2 loss   -reboot                     </td><td> L2-A is active, L2-B is mirror. All systems are running and available to take traffic</td><td>1. Run app     2. Reboot L2-B (mirror)</td><td>data directory needs to be cleaned up, then when L2-B is restarted,  it re-synchs state from Active Server.</td></tr>
<tr><td>TPL5</td><td>Mirror L2 loss   -Pull Plug            </td><td> L2-A is active, L2-B is mirror. All systems are running and available to take traffic</td><td>1. Run app     2. Pull plug on  L2-B (mirror)</td><td>data directory needs to be cleaned up, then when L2-B is restarted,  it re-synchs state from Active Server.</td></tr>
</table>

<h4 id="test-plan---failoverfailback-tests">Test Plan - Failover/Failback Tests</h4>

<p>This section outlines tests to confirm the cluster ability to fail-over to the mirror Terracotta server, and fail back.</p>

<p>The test plan for testing fail over and fail back consists of the following tests:</p>

<table>
<tr><th>TestID</th><th> Test </th><th> Setup </th><th> Steps </th><th> Expected Result </th></tr>
<tr><td>TFO1</td><td>Failover/Failback</td><td> L2-A is active, L2-B is mirror. All systems are running and available to take traffic</td><td>1. Run application     2. Kill -9 (or run stop-tc-server) on L2-A (Active)                                                  3. After L2-B takes over as Active, start-tc-server on L2-A.   (L2-A is now mirror) 4. Kill -9 (or run stop-tc-server) on L2-B. (L2-A is now Active)</td><td>After first failover L2-A-&gt;L2-B, txns should continue. L2-A should come up cleanly in mirror mode when tc-server is run.  When second failover occurs L2-B-&gt;L2-A, L2-A should process txns.</td></tr>
</table>

<h4 id="test-plan---loss-of-switch-tests">Test Plan - Loss of Switch Tests</h4>

<p>{tip}
This test can only be run on a redundant network
{tip}</p>

<p>This section outlines testing the loss of a switch in a redundant network, and confirming that no interrupt of service occurs.</p>

<p>The test plan for testing failure of a single switch consists of the following tests:</p>

<table>
<tr><th>TestID</th><th> Test </th><th> Setup </th><th> Steps </th><th> Expected Result </th></tr>
<tr><td>TSL1</td><td>Loss of 1 Switch </td><td> 2 Switches in redundant configuration. L2-A is active, L2-B is mirror. All systems are running and available to take traffic.</td><td>1. Run application   2. Power down/pull plug on Switch</td><td>All traffic transparently moves to switch 2 with no interruptions</td></tr>
</table>

<h4 id="test-plan---loss-of-network-connectivity">Test Plan - Loss of Network Connectivity</h4>

<p>This section outlines testing the loss of network connectivity.</p>

<p>The test plan for testing failure of the network consists of the following tests:</p>

<table>
<tr><th>TestID</th><th> Test </th><th> Setup </th><th> Steps </th><th> Expected Result </th></tr>
<tr><td>TNL1</td><td>Loss of NIC wiring (Active)             </td><td> L2-A is active, L2-B is mirror. All systems are runnng and available to traffic</td><td>1. Run application     2. Remove Network Cable on L2-A</td><td>All traffic transparently moves to L2-B with no interruptions</td></tr>
<tr><td>TNL2</td><td>Loss of NIC wiring (mirror)             </td><td> L2-A is active, L2-B is mirror. All systems are runnng and available to traffic</td><td>1. Run application     2. Remove Network Cable on L2-B</td><td>No user impact on cluster</td></tr>
</table>

<h4 id="test-plan---terracotta-cluster-failure">Test Plan - Terracotta Cluster Failure</h4>

<p>This section outlines the tests to confirm successful continued operations in the face Terracotta Cluster failures.</p>

<p>The test plan for testing Terracotta Cluster failures consists of the following tests:</p>

<table>
<tr><th>TestID </th><th> Test </th><th> Setup </th><th> Steps </th><th> Expected Result </th></tr>
<tr><td>TF1</td><td>Process Failure Recovery             </td><td> L2-A is active, L2-B is mirror. All systems are runnng and available to traffic</td><td>1. Run application     2. Bring down all L1s and L2s                 3. Start L2s then L1s</td><td>Cluster should come up and begin taking txns again</td></tr>
<tr><td>TF2</td><td>Server Failure Recovery             </td><td> L2-A is active, L2-B is mirror. All systems are runnng and available to traffic</td><td>1. Run application     2. Power down all machines                           3. Start L2s and then L1s</td><td>Should be able to run application once all servers are up.</td></tr>
</table>

<h4 id="client-failure-tests">Client Failure Tests</h4>

<p>This section outlines tests to confirm successful continued operations in the face of Terracotta client failures.</p>

<p>The test plan for testing Terracotta Client failures consists of the following tests:</p>

<table>
<tr><th>TestID</th><th> Test </th><th> Setup </th><th> Steps </th><th> Expected Result </th></tr>
<tr><td>TCF1</td><td>L1 Failure -         </td><td> L2-A is active, L2-B is mirror.                                        2 L1s L1-A and L1-B                     All systems are running and available to traffic</td><td>1. Run application     2. kill -9 L1-A.                                        </td><td>L1-B should take all incoming traffic.                Some timeouts may occur due to txns in process when L1 fails over. </td></tr>
</table>

      </article>
    </div>

  </div>

</div>

      </div>
    </div>

    <br/>
<footer class="site-footer">

  <div class="container">

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        Related Projects:<br/>
        <a href="http://www.ehcache.org"><img src="/images/ehcache.png" style=""></a><br/><br/>
        <a href="http://www.quartz-scheduler.org"><img src="/images/logo-quartz-scheduler.png" style="max-height: 32px;"></a>

        <!--
        <ul class="contact-list">
          <li>Terracotta</li>
          <li><a href="mailto:tc-oss@softwareag.com">tc-oss@softwareag.com</a></li>
        </ul>
      -->
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/terracotta-oss">
              <i class="fa fa-github"></i> GitHub
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/terracottatech">
              <i class="fa fa-twitter"></i> Twitter
            </a>
          </li>
          

          
          <li>
            <a href="http://www.facebook.com/Terracotta">
              <i class="fa fa-facebook"></i> Facebook
            </a>
          </li>
          

          
          <li>
            <a href="http://www.linkedin.com/company/terracotta">
              <i class="fa fa-linkedin"></i> LinkedIn
            </a>
          </li>
          

          <li>
            <a href="/feed.xml" title="Atom/RSS Feed">
              <i class="fa fa-rss-square"></i> Atom/RSS Feed
            </a>
          </li>
        </ul>
      </div>

    <div class="footer-col  footer-col-3">
      <a href="/downloads/"><i class="fa fa-download"></i> Download Now</a>
      <br/>
      <a href="/documentation/"><i class="fa fa-book"></i> Documentation</a>
      <br/>
      <a href="/resources/"><i class="fa fa-external-link-square"></i> Resources</a>
      <br/>
      <a href="/blog/"><i class="fa fa-rss-square"></i> Terracotat Blog</a>
      <br/>
      <a href="/community/"><i class="fa fa-users"></i> Join the Community</a>
    </div>

    </div>

    <div class="container-fluid">
        <hr/>
        <a class="SAG-Logo" href="http://www.softwareag.com/corporate/products/terracotta"><img src="/images/SAG_Logo.png" style="height:1.5em;width:9em;"/></a>
        <div class="footer-text">
          <em class="copyleft">Terracotta is Open Source and freely available under the Terracotta Public License 2.0</em>
          <br/>
          <em class="copyright">&copy; Terracotta, Inc., a wholly-owned subsidiary of Software AG USA, Inc. All rights reserved.</em>
        </div>
    </div>
  </div>

</footer>

<!-- Bootstrap core JavaScript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
<!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
<script src="https://maxcdn.bootstrapcdn.com/js/ie10-viewport-bug-workaround.js"></script>

<!--  <script type="text/javascript" src="http://cdnjs.cloudflare.com/ajax/libs/jquery-scrollTo/2.1.0/jquery.scrollTo.min.js"/> -->

<script type="text/javascript">
        $('#').addClass("active");
        $('#').addClass("active");
</script>


  </body>

</html>
